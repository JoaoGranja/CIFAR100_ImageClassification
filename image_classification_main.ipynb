{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"image_classification_main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/JoaoGranja/CIFAR100_ImageClassification/blob/master/image_classification_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"fug7eGzTzLbg","executionInfo":{"status":"ok","timestamp":1631630588808,"user_tz":-60,"elapsed":428,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","is_colab = True"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PycgYA0ppQa"},"source":["\n","# **Colab Preparation** \n","Before handling the project, we need to install tensorflow/keras and pip packages. I also share my google drive to simplify the connection with my google drive account.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8j5egONdppQb","executionInfo":{"status":"ok","timestamp":1631630611740,"user_tz":-60,"elapsed":22936,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}},"outputId":"35636f7f-dd08-4fe4-b89b-840325fd47bc"},"source":["if is_colab:\n","    #Package Installation and share Google Drive\n","    !pip install --upgrade pip\n","    #!pip install --upgrade keras\n","    !pip install keras-resnet\n","    !pip install tensorflow==2.4.0\n","    !pip install tensorflow-gpu==2.4.0\n","    !pip install keras==2.4.0\n","\n","    #!pip install --upgrade tensorflow_hub\n","    !pip install tensorflow_addons\n","    !pip install --quiet vit-keras\n","\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.2.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: keras-resnet in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from keras-resnet) (2.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet) (3.13)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet) (2.4.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet) (1.4.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.4->keras-resnet) (2.10.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (2.4.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.15.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.6.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.1.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.3.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.12.1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.37.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.17.3)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (2.6.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.1.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.3.3)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.2.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.12.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.32.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.7.4.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.4.5)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.34.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.3.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras>=2.2.4->keras-resnet) (3.5.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: tensorflow==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.32.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.6.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.10.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.3.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.7.4.3)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.12.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12.1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.4.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.34.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.5)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2021.5.30)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.5.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: tensorflow-gpu==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.1.0)\n","Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (2.10.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.12.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.6.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.12.1)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.1.2)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.3.3)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.19.5)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.3.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.7.4.3)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.12)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.2.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.37.0)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (2.4.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.32.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.17.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.15.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.23.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.34.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.5)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (57.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.5.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: keras==2.4.0 in /usr/local/lib/python3.7/dist-packages (2.4.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.4.1)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.4.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.19.5)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.17.3)\n","Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.4.0)\n","Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.32.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.3.3)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.6.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.37.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.12.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.5)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.34.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.23.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.5.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.14.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"NToyjnZ4Uc2k"},"source":[" # **Configuration and imports**\n","\n","The first thing I need to do is to import all modules we need for this project. I also use some configuration parameters to provide more flexibility to the tranining process\n","\n","In this project I will be making use of the Keras library for creating our model and training it. I will also use Matplotlib for visualizing our dataset to gain a better understanding of the images we are going to be handling.\n","\n"]},{"cell_type":"code","metadata":{"id":"Rk78b8REzLbl","executionInfo":{"status":"ok","timestamp":1631630613868,"user_tz":-60,"elapsed":2153,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["# Generic Imports\n","import time\n","import gc\n","import logging, os\n","import sys\n","import random\n","import warnings\n","import pickle\n","from math import ceil\n","from tqdm import tqdm\n","from itertools import chain\n","\n","# data processing and visualization library\n","import numpy as np\n","import pandas as pd\n","#import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# image procesing library\n","#from skimage.io import imread, imshow, imread_collection, concatenate_images\n","#from skimage.transform import resize\n","#from skimage.morphology import label\n","import cv2 \n","\n","# tensorflow and keras for CNN model\n","import tensorflow as tf\n","logging.disable(logging.WARNING)\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","from tensorflow.keras.models import Model, save_model, load_model, Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n","from tensorflow.keras.datasets import cifar100\n","from tensorflow.keras.utils import to_categorical\n","#from keras.utils import multi_gpu_model\n","from tensorflow.keras.preprocessing.image import Iterator, load_img, img_to_array, ImageDataGenerator\n","\n","if is_colab:\n","    from google.colab import files\n","    sys.path.append('/content/drive/MyDrive/colab/CIFAR100_Image_Classification')\n","    os.chdir('/content/drive/MyDrive/colab/CIFAR100_Image_Classification')\n","    \n","#------------------------------  Set some configuration parameters -----------------------------------#\n","args = {}\n","args['seed'] = 42\n","args['data_augmentation'] = True\n","args['training'] = True\n","args['fine_tuning'] = True\n","args['evaluation_networks'] = False\n","\n","#training arguments\n","args['batch_size'] = 32\n","args['train_epochs'] = 10\n","args['fine_tune_epochs'] = 5\n","#args['validation_split'] = 0.2\n","\n","#model arguments\n","args['compare_networks'] = ['vgg19', 'resnet50', 'efficientnetb0', 'efficientnetv2', 'vit-b32', 'vit_scratch']\n","args['network'] = 'vgg19'\n","args['models_dir'] = 'nn_models_checkpoints'\n","\n","#optimizer arguments\n","args['optimizer'] = 'adam'\n","#args['weights'] = 'nn_models_checkpoints/best_{}.h5'.format(args['networks'][0])\n","args['learning_rate'] = 0.001\n","args['decay'] = 0.0001\n","args['loss'] = 'categorical_crossentropy'\n","\n","# In this project I will also train a vit model from scratch. So I will use more epochs for training....\n","if args['network'] == 'vit_scratch':\n","  args['batch_size'] = 256\n","  args['train_epochs'] = 20  \n","\n","warnings.filterwarnings('ignore', category=UserWarning, module='skimage')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjI2CcCKppQe"},"source":["## Loading the dataset\n","\n","In this project I will use the CIFAR100 dataset which is comprised of 60000 32x32 color images in 100 classes, with 600 images per class. There are 50000 training images and 10000 test images. More information is available on [CIFAR homepage](https://www.cs.toronto.edu/~kriz/cifar.html)"]},{"cell_type":"code","metadata":{"id":"32RfKAg6ppQf","executionInfo":{"status":"ok","timestamp":1631630613869,"user_tz":-60,"elapsed":9,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["def unpickle(file):\n","    with open(file, 'rb') as data:\n","        dataset = pickle.load(data)\n","    return dataset"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_T8_V1PqppQg","executionInfo":{"status":"ok","timestamp":1631630616242,"user_tz":-60,"elapsed":2382,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}},"outputId":"90c3af9b-612a-4dde-ea21-ff46d53b4ac2"},"source":["#------------------------------  Load dataset using keras.dataset API -----------------------------------#\n","download_dataset = True\n","if download_dataset:\n","    train_ds, test_ds = cifar100.load_data()\n","    dataset = {}\n","    dataset['train_ds'], dataset['test_ds'] = train_ds, test_ds\n","    with open('dataset/cifar100.pickle', 'wb') as output:\n","        pickle.dump(dataset, output)\n","else:\n","    dataset = unpickle('dataset/cifar100.pickle')\n","    train_ds, test_ds = dataset['train_ds'], dataset['test_ds']\n","\n","#Check train and test dataset shape\n","x_train, y_train = train_ds\n","print(\"Train dataset: x={} y={}\".format(x_train.shape, y_train.shape))\n","#Check number of classes and image shape\n","print(\"Image data shape =\", x_train.shape[1:])\n","nr_classes = len(np.unique(y_train))\n","print(\"Number of classes =\", nr_classes )\n","0\n","x_test, y_test = test_ds\n","print(\"Test dataset: x={} y={}\".format(x_test.shape, y_test.shape))\n","   "],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset: x=(50000, 32, 32, 3) y=(50000, 1)\n","Image data shape = (32, 32, 3)\n","Number of classes = 100\n","Test dataset: x=(10000, 32, 32, 3) y=(10000, 1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"2v20CtzuppQj","executionInfo":{"status":"ok","timestamp":1631630616894,"user_tz":-60,"elapsed":662,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}},"outputId":"d6046142-7fcb-45f5-8d8d-f3d4c35f549e"},"source":["#------------------------------ Plot random images and respective labels -----------------------------------#\n","n_images = 5\n","fig, axs = plt.subplots(1, n_images, figsize=(12, 12))\n","fig.tight_layout(pad=1.0)\n","   \n","for i in range(n_images):\n","    index = random.randint(0, len(x_train))\n","    image = x_train[index].squeeze()\n","\n","    axs[i].imshow(image)\n","    axs[i].set_title(\"Label = {0}\".format(int(y_train[index])))\n","    axs[i].get_xaxis().set_visible(False)\n","    axs[i].get_yaxis().set_visible(False)\n"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x864 with 5 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"hLijVktappQk"},"source":["Analyzing the images it is clear that the images resolution is small, actually 32x32 has few pixels and therefore can be a challenge for a model to classify correctly the object. Furthermore all image has the same size so it is not required to resize the input images."]},{"cell_type":"markdown","metadata":{"id":"Q9i_voo8zLbs"},"source":["# **Data pre-processing and data augmentation**\n","The dataset output ranges from class 0 to 99. For training a model, it is easier to use a one hot encoding for the class element of each sample, in this way we transform any integer into a 100 element binary vector with a 1 for the index of the class value.\n","\n","In order to improve the model generalization for new unseen images, I will also apply data augmentation process based on random transformations (rotation, image flip, image shift and zooming)"]},{"cell_type":"code","metadata":{"id":"YZGByJN1ppQo","executionInfo":{"status":"ok","timestamp":1631630616895,"user_tz":-60,"elapsed":21,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["#------------------------------ Categorical Transformation  -----------------------------------#\n","y_train = to_categorical(y_train, nr_classes)\n","y_test = to_categorical(y_test, nr_classes)\n","\n","\n","#------------------------------ Random data augmentation -----------------------------------#\n","if args['data_augmentation']:\n","    train_datagen = ImageDataGenerator(\n","                        rotation_range=20,\n","                        width_shift_range=0.2,\n","                        height_shift_range=0.2,\n","                        zoom_range=0.2,\n","                        horizontal_flip=True)\n","                        #validation_split=args['validation_split'])\n","    # prepare iterator\n","    train_generator = train_datagen.flow(x_train, y_train, batch_size=args['batch_size']) #, subset='training')\n","    #validation_generator = train_datagen.flow(x_train, y_train, batch_size=args['batch_size'], subset='validation')\n","    \n","else:\n","    train_datagen = ImageDataGenerator()\n","    train_generator = train_datagen.flow(x_train, y_train, batch_size=args['batch_size'])\n","    #validation_generator = train_datagen.flow(x_train, y_train, batch_size=args['batch_size'], subset='validation')\n","\n","test_datagen = ImageDataGenerator()\n","test_generator = test_datagen.flow(x_test, y_test, batch_size=args['batch_size'])\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_QCnufRUl-Z5"},"source":["# **Optimizer**\n","\n","Before training it is necessary to choose an optimizer which will be responsible to adjust model parameters in order to reduce the loss funcion"]},{"cell_type":"code","metadata":{"id":"INAQZOJSphOA","executionInfo":{"status":"ok","timestamp":1631630616896,"user_tz":-60,"elapsed":20,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["#------------------------------ Define an optimizer -----------------------------------#\n","if 'optimizer' in args:\n","    if args['optimizer'] == 'rmsprop':\n","        optimizer = RMSprop(learning_rate=args['learning_rate'], decay=float(args['decay']))\n","    elif args['optimizer'] == 'adam':\n","        optimizer = Adam(learning_rate=args['learning_rate'], decay=float(args['decay']))\n","    elif args['optimizer'] == 'amsgrad':\n","        optimizer = Adam(learning_rate=args['learning_rate'], decay=float(args['decay']), amsgrad=True)\n","    elif args['optimizer'] == 'sgd':\n","        optimizer = SGD(learning_rate=args['learning_rate'], momentum=0.9, nesterov=True, decay=float(args['decay']))\n","else:\n","    optimizer = RMSprop(learning_rate=args['learning_rate'])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWLJo5_bppQs"},"source":["# **Model**\n","\n","In this project, I will use several Deep Learning Models for further comparison their performance. All models are defined on the separate module \"models\".\n","\n","State of the art model architectures will be applied, such as VGG19, Resnet50, EfficientNet but also the most Deep Learning model architecture, ViT model.\n","\n","To improve the results, transfer learning (mainly from Imagenet dataset checkpoints) will be used except for the 'vit_cratch' model, where as the name suggests, I will train a ViT model from scratch. \n","\n","When transfer learning is applied, the backbone model will be configured as not trainable to just train the last classifier for this specific dataset. After that a soft fine-tuning will be processed for the last backbone model layers.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gd5VM-R-Su0_","executionInfo":{"status":"ok","timestamp":1631630619667,"user_tz":-60,"elapsed":2790,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}},"outputId":"214cd37d-2c7b-4b14-ae04-1b63f514e4a6"},"source":["from models.model_factory import make_model\n","if args['training']:\n","  #------------------------------ Make the model -----------------------------------#\n","  model = make_model(args['network'], x_train.shape[1:], nr_classes)\n","\n","  if 'weights' not in args:\n","      print('No weights passed, training from scratch')\n","  else:\n","      print('Loading weights from {}'.format(args['weights']))\n","      model.load_weights(args['weights'], by_name=True)\n","\n","  #------------------------------ Compile the model -----------------------------------#\n","  model.compile(loss=args['loss'], optimizer=optimizer, metrics=['accuracy']) \n","  model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 1s 0us/step\n","No weights passed, training from scratch\n","Model: \"VGG19\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n","_________________________________________________________________\n","lambda (Lambda)              (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","tf.__operators__.getitem (Sl (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","vgg19 (Functional)           (None, 7, 7, 512)         20024384  \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 512)               2048      \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               262656    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 512)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 100)               25700     \n","=================================================================\n","Total params: 20,448,164\n","Trainable params: 421,732\n","Non-trainable params: 20,026,432\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"mSADHpegzLb-"},"source":["# **Training**\n","\n","This step called training comprises on fitting the model parameters to classify correcty the images. For that, the optimizer uses a loss function to quantify the discrepance between true and predicted labels. Based on that it adjusts the model parameters to decrease the loss. \n","\n","Model checkpoints, EarlyStopping and Learning Rate reduce are used as callbacks to improce training efficiency."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"cSWGGh32Svnu","executionInfo":{"status":"error","timestamp":1631630700881,"user_tz":-60,"elapsed":81229,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}},"outputId":"6207f384-1199-4740-e850-cd6afe0059c8"},"source":["if args['training']:\n","    print(\"Training model {}\".format(model.name))\n","    #------------------------------ Model check points -----------------------------------#\n","    best_model_file = '{}/best_{}.h5'.format(args['models_dir'], model.name)\n","    last_model_file = '{}/last_{}.h5'.format(args['models_dir'], model.name)\n","    model_file = '{}/model_{}.h5'.format(args['models_dir'], model.name)\n","\n","    #------------------------------ Callbacks -----------------------------------#\n","\n","    #ModelCheckpoint(filepath=last_model_file, monitor='val_loss', verbose=1, mode='min',\n","    #            save_freq='epoch', save_best_only=False, save_weights_only=False)\n","\n","    callbacks = [\n","            # Callback to reduce the learning rate once the plateau has been reached:\n","            ReduceLROnPlateau(\n","                monitor='val_loss',\n","                min_delta=0.1,\n","                factor=1/3,\n","                patience=2,\n","                mode='auto',\n","                verbose=1,\n","                cooldown=0,\n","                min_lr=1e-8\n","            ),\n","            # Callback to stop the training once no more improvements are recorded:\n","            EarlyStopping(\n","                min_delta=0.001,\n","                verbose=1,\n","                patience=10,\n","                mode='auto',\n","                restore_best_weights=True\n","            ),\n","            # Callback to log the graph, losses and metrics into TensorBoard:\n","            TensorBoard(log_dir=\"logs/{}\".format(model.name)\n","            ),\n","            # Callback to save the best model specifying the epoch and val-loss in the filename:\n","            ModelCheckpoint(filepath=best_model_file, \n","                monitor='val_loss',\n","                verbose=1,\n","                mode='min',\n","                save_freq='epoch',\n","                save_best_only=True,\n","                save_weights_only=False)\n","        ]\n","\n","    #------------------------------ Model Fit -----------------------------------#\n","    steps = len(x_train) // args['batch_size']\n","    history = model.fit(\n","                        train_generator,\n","                        steps_per_epoch=steps,\n","                        epochs=args['train_epochs'],\n","                        validation_data=test_generator,\n","                        callbacks=callbacks)\n","\n","    #------------------------------ Save the last model weights -----------------------------------#\n","    model.save(model_file)\n","    print(\"Saved model to disk\") "],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model VGG19\n","Epoch 1/10\n"," 116/1562 [=>............................] - ETA: 8:32 - loss: 4.6001 - accuracy: 0.0613"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a4d29855fe2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#------------------------------ Save the last model weights -----------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"szQZCm1WzprW"},"source":["# **Fine tuning**\n","\n","In the feature extraction experiment, you were only training a few layers on top of the pre-trained base model. The weights of the pre-trained network were not updated during training. \n","\n","One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset. \n","\n","In fine-tune process, it is important to choose a small learning rate for the optimizer."]},{"cell_type":"code","metadata":{"id":"KRobxT6d0DeF","executionInfo":{"status":"aborted","timestamp":1631630700876,"user_tz":-60,"elapsed":6,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["if args['fine_tuning']:\n","  if (args['network']  != 'vit_scratch'):\n","    base_model = model.get_layer(args['network'])\n","    base_model.trainable = True\n","\n","  if (args['network']  in   ['vgg19', 'resnet50', 'efficientnetb0']):\n","    # Let's take a look to see how many layers are in the base model\n","    print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","    # Fine-tune from this layer onwards\n","    fine_tune_at = len(base_model.layers) // 2\n","\n","    # Freeze all the layers before the `fine_tune_at` layer\n","    for layer in base_model.layers[:fine_tune_at]:\n","      layer.trainable =  False\n","\n","  #------------------------------ Compile the model -----------------------------------#\n","  model.compile(loss=args['loss'], optimizer=RMSprop(learning_rate=args['learning_rate']/100), metrics=['accuracy']) \n","  model.summary()\n","\n","  #------------------------------ Model Fit -----------------------------------#\n","  steps = len(x_train) // args['batch_size']\n","  total_epochs =  args['train_epochs'] + args['fine_tune_epochs']\n","  fine_tune_history = model.fit(\n","                      train_generator,\n","                      steps_per_epoch=steps,\n","                      epochs=total_epochs,\n","                      initial_epoch = history.epoch[-1],\n","                      validation_data=test_generator,\n","                      callbacks=callbacks)\n","\n","  #------------------------------ Save the last model weights -----------------------------------#\n","  model.save(model_file)\n","  print(\"Saved model to disk\")    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lI4rNBwYgXwv"},"source":["# **Visualise Model Results**"]},{"cell_type":"code","metadata":{"id":"-tUF9-UtppQx","executionInfo":{"status":"aborted","timestamp":1631630700877,"user_tz":-60,"elapsed":7,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["#------------------------------ Plot diagnostic learning curves -----------------------------------#\n","def summarize_diagnostics(history, model_name, fine_tune_history={}):\n","    # plot loss\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    if args['fine_tuning']:\n","      acc += fine_tune_history.history['accuracy']\n","      val_acc += fine_tune_history.history['val_accuracy']\n","\n","      loss += fine_tune_history.history['loss']\n","      val_loss += fine_tune_history.history['val_loss']  \n","\n","    plt.figure(figsize=(8, 8))\n","    plt.subplot(2,1,1)\n","    plt.plot(acc, label='Training Accuracy')\n","    plt.plot(val_acc, label='Validation Accuracy')\n","    plt.ylim([0.1, 1])\n","    if args['fine_tuning']:\n","      plt.plot([args['train_epochs'],args['train_epochs']],\n","          plt.ylim(), label='Start Fine Tuning')\n","    plt.legend(loc='lower right')\n","    plt.title('Training and Validation Accuracy')\n","\n","    plt.subplot(2,1,2)\n","    plt.plot(loss, label='Training Loss')\n","    plt.plot(val_loss, label='Validation Loss')\n","    #plt.ylim([0, 10])\n","    if args['fine_tuning']:\n","      plt.plot([args['train_epochs'],args['train_epochs']],\n","          plt.ylim(), label='Start Fine Tuning')\n","    plt.legend(loc='upper right')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('epoch')\n","    \n","    # save plot to file\n","    plt.savefig(\"results_\" + model_name  + '_plot.png')\n","    plt.show()\n","    plt.close()\n","\n","if args['training']:  \n","    if args['fine_tuning']:\n","      summarize_diagnostics(history, args['network'], fine_tune_history)\n","    else:\n","      summarize_diagnostics(history, args['network'])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rg9NFaPNzLcH"},"source":["# **Evaluation**\n","\n","Evaluate the model over the test dataset. We will use the last model weights and predict the class for some test images"]},{"cell_type":"code","metadata":{"id":"_HGuxPkippQy","executionInfo":{"status":"aborted","timestamp":1631630700880,"user_tz":-60,"elapsed":10,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":["gc.collect\n","if args['evaluation_networks']:\n","\n","  def show_image_prediction(loaded_model, n_images):\n","    #Try out the model on an image from the test data:\n","    #plt.figure(figsize=(30, 30))\n","    fig, axs = plt.subplots(1, n_images, figsize=(15, 15))\n","    fig.tight_layout(pad=1.0)\n","\n","    # View the images\n","    for i in range(n_images):\n","        index = random.randint(0, len(x_test))\n","        image = x_test[index].squeeze()\n","        true_index = [i for i in range(nr_classes) if y_test[index][i] == 1 ][0]\n","\n","        prediction_scores = loaded_model.predict(np.expand_dims(image, axis=0))\n","        predicted_index = np.argmax(prediction_scores)\n","\n","        #image = np.add(image*128,128).astype(int)\n","        axs[i].imshow(image)\n","        axs[i].set_title(\"True Label = {0}, \\n Predicted label = {1}\".format(true_index, predicted_index))\n","        axs[i].get_xaxis().set_visible(False)\n","        axs[i].get_yaxis().set_visible(False)\n","    \n","    plt.show()\n","    plt.close()\n","\n","  #------------------------------ Predict on some test images -----------------------------------#\n","  i=1\n","\n","  for network in args['compare_networks']:\n","      print(\"----------------------- model {} -----------------------\".format(network))\n","\n","\n","      #plt.subplot(1, len(args['networks']), i)\n","      #Load last model parameters\n","      last_model_file = '{}/last_{}.h5'.format(args['models_dir'], network)\n","      best_model_file = '{}/best_{}.h5'.format(args['models_dir'], network)\n","      model_file = '{}/model_{}.h5'.format(args['models_dir'], network)\n","\n","      #Load the model\n","      loaded_model = load_model(model_file)\n","      print(\"Loaded model {} from disk\".format(model_file))\n","      loaded_model.compile(loss=args['loss'], optimizer=optimizer, metrics=['accuracy']) \n","\n","      #Try out the model on an image from the test data:\n","      show_image_prediction(loaded_model,8)\n","\n","      #------------------------------ Evaluate model on testing dataset -----------------------------------#\n","      #_, acc = loaded_model.evaluate(x_train, y_train, verbose=0)\n","      #print(\"Training accuracy of model {0} = {1}\".format(loaded_model.name, acc))\n","      _, acc = loaded_model.evaluate(x_test, y_test, verbose=0)\n","      print(\"Testing accuracy of model {0} = {1}\".format(loaded_model.name, acc)) \n","\n","      #------------------------------ Plot all model results -----------------------------------#\n","      # save plot to file\n","      filename = \"accuracy_\" + network + '_plot.png'\n","      im = cv2.imread(filename)\n","      plt.imshow(im)\n","      plt.title(\"Accuray Results of model {}\".format(network))\n","      i = i + 1\n","\n","      plt.show()\n","  plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GfrD3I1TYU8L"},"source":["Comparing the results obtained from the three models *LeNet*, *VGG16* and *VGG19*, it is possible to conclude the model with the best result is *VGG16* followed by *VGG19*. Indeed *VGG16* has the highest accuracy evaluated on testing dataset. Due to reduced capacity, *LeNet* starts overfiting earlier, actually the fine-tuning process didn't improve model results."]},{"cell_type":"markdown","metadata":{"id":"K6ajPGnahFrv"},"source":["**Conclusion**\n","\n","In this project I built from scratch a simple LeNet model and used 2 pre-trained models, VGG16 and VGG19. I train all models and evaluate the results against the CIFAR10 testing dataset. Taking into account the accuracy, the best model for this dataset was VGG16. \n","\n","So in this project, I applied several workflow steps to train a deep learning model on the image classification problem. In sume the steps were: pre-processing the input data, apply data-augmentation, build a CNN model, use pre-trained models, train a model, fine-tuning the model and evaluate the model. \n","\n","I used transfer learning technique on VGG16 and VGG19 models which is usefull when our dataset is not so large like in this case. The models accuracy on testing dataset is around 81/83% which are good results. \n","\n","For future work, I could apply better data augmentation in order to avoid overfitting as well as regularization techniques like L1 or L2. "]},{"cell_type":"code","metadata":{"id":"3Q9aWqIvhGMm","executionInfo":{"status":"aborted","timestamp":1631630700880,"user_tz":-60,"elapsed":9,"user":{"displayName":"Granja João","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_pc30wvr--zKG1D_7yoyGBIwlFi5nbstwDsGYsxw=s64","userId":"15209494838847442457"}}},"source":[""],"execution_count":null,"outputs":[]}]}